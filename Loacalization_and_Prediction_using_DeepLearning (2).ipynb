{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Loacalization and Prediction using DeepLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uffrquw8YXlw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KdJMRmvFykb"
      },
      "source": [
        "#  localization and locatoion prediction with deep learning\n",
        "\n",
        "Since GPS signals can be unreliable this project uses wifi fingerprints in order to predict the location of users. Data: 19,937 training & 1111 validation datapoints with 529 attributes (wifi fingerprints) measured by 25 different android devices.Each WiFi fingerprint can be characterized by the detected Wireless Access Points (WAPs) and the corresponding Received Signal Strength Intensity (RSSI). The intensity values are represented as negative integer values ranging -104dBm (extremely poor signal) to 0dbM. The positive value 100 is used to denote when a WAP was not detected. During the database creation, 520 different WAPs were detected. Thus, the WiFi fingerprint is composed by 520 intensity values.\n",
        "\n",
        "Then the coordinates (latitude, longitude, floor) and Building ID are provided as the attributes to be predicted.\n",
        "Relative positioning and latitude and longitude prediction is the first step to modelling a succesfull location based ad campaign,followed by maping the actual area where the target(mobile devise to which the advert will be shown) then choosing the appropriate ad.\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        " ## LINVAL T CHISOKO R181563R\n",
        " ## MAKOMBORERO MAGAYA R181571B\n",
        " ## SAMANTHA MUGARI R181725N\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I2XYvmfMMDG"
      },
      "source": [
        "# Modules Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q8whI0kFR1h"
      },
      "source": [
        "# Imports\n",
        "\n",
        "# necessary Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import pprint\n",
        "\n",
        "#Visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "#Preprocessing\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA \n",
        "from scipy.sparse import lil_matrix\n",
        "\n",
        "#Scoring Metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgcg0Z19eij0"
      },
      "source": [
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import os\n",
        "from scipy.stats import zscore               # min max scaler\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from matplotlib.pyplot import figure, show\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Plot a confusion matrix.\n",
        "# cm is the confusion matrix, names are the names of the classes.\n",
        "def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(names))\n",
        "    plt.xticks(tick_marks, names, rotation=45)\n",
        "    plt.yticks(tick_marks, names)\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    \n",
        "\n",
        "# Plot an ROC. pred - the predictions, y - the expected output.\n",
        "def plot_roc(pred,y):\n",
        "    fpr, tpr, thresholds = roc_curve(y, pred)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC)')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    \n",
        "# imports for tf and other models\n",
        "\n",
        "import collections\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "\n",
        "def to_xy(df, target):\n",
        "    result = []\n",
        "    for x in df.columns:\n",
        "        if x != target:\n",
        "            result.append(x)\n",
        "    # find out the type of the target column. \n",
        "    target_type = df[target].dtypes\n",
        "    target_type = target_type[0] if isinstance(target_type, collections.Sequence) else target_type\n",
        "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
        "    if target_type in (np.int64, np.int32, np.object):\n",
        "        # Classification\n",
        "        dummies = pd.get_dummies(df[target])\n",
        "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
        "    else:\n",
        "        # Regression\n",
        "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)\n",
        "    \n",
        "def encode_text_index(df, name):\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    df[name] = le.fit_transform(df[name])\n",
        "    return le.classes_\n",
        "\n",
        "def chart_regression(pred,y,sort=True):\n",
        "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
        "    if sort:\n",
        "        t.sort_values(by=['y'],inplace=True)\n",
        "    a = plt.plot(t['y'].tolist(),label='expected')\n",
        "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
        "    plt.ylabel('output')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "from sklearn.metrics import r2_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAZAhVzfe8gs"
      },
      "source": [
        "import keras\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "import sklearn.feature_extraction.text as tfidf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets,linear_model, preprocessing,utils\n",
        "from sklearn.metrics import mean_squared_error,r2_score\n",
        "from scipy.stats import zscore\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import f1_score\n",
        "import collections\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras import optimizers\n",
        "from keras.layers import Conv1D, Conv2D, MaxPooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PFnWd77Hbhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5edb75e3-fc35-4d47-e54a-fda63bf55498"
      },
      "source": [
        "# Mount Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXaGtB3nMRfv"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3URWomnqHfol"
      },
      "source": [
        "# Loading raw data \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsLUHeyoDsmD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "bc42906e-a31f-47f0-8a36-60cb4a692504"
      },
      "source": [
        "# path to the dataset\n",
        "train_data = pd.read_csv(\"drive/My Drive/kbs/trainingData.csv\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f1dac48a5ad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# path to the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/My Drive/kbs/trainingData.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/My Drive/kbs/trainingData.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNdY6HH8MZp1"
      },
      "source": [
        "# Knowing the Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ4GkFUFImKr"
      },
      "source": [
        "# Dataset shape\n",
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edw81wTBKgzP"
      },
      "source": [
        "train_data.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qe42z4iLoyw"
      },
      "source": [
        "#  getting the Datatypes in the dataset\n",
        "list(train_data.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93JnhefFYpAG"
      },
      "source": [
        "print(list(train_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smak0w-zzZiG"
      },
      "source": [
        "feature_list = list(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey1NcXaSC5U8"
      },
      "source": [
        "#Drop unneeded data\n",
        "train_data.drop(['USERID', 'PHONEID', 'TIMESTAMP'], axis = 1, inplace=True)\n",
        "\n",
        "#Remove \"NaN\" value\n",
        "col = train_data.columns[0:520]\n",
        "for i in col:\n",
        "    train_data[i].fillna(0, inplace=True)\n",
        "train_data.dropna(subset=['LONGITUDE','LATITUDE', 'FLOOR', 'BUILDINGID' ], inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "#Processing \"WAP\" data\n",
        "train_data.iloc[:, 0:520] = np.where(train_data.iloc[:, 0:520] <= 0, \n",
        "                        train_data.iloc[:, 0:520] + 105, \n",
        "                        train_data.iloc[:, 0:520] - 100)\n",
        "\n",
        "#Processing Longtitudinal data\n",
        "train_data.iloc[:, 520] = np.where(train_data.iloc[:, 520] <= 0, \n",
        "                        -train_data.iloc[:, 520], \n",
        "                        train_data.iloc[:, 520])\n",
        "\n",
        "\n",
        "train_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ_w7HtjDRi7"
      },
      "source": [
        "min_LGT = 7300.818990\n",
        "min_LAT = 4.864746e+06\n",
        "\n",
        "train_data.iloc[:,520] = (train_data.iloc[:, 520] - min_LGT + 1)\n",
        "train_data.iloc[:,521] = (train_data.iloc[:, 521] - min_LAT + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huyOeNAVSS_r"
      },
      "source": [
        "# Splitting the dataset into x and y\n",
        "\n",
        "out_data = train_data.copy()\n",
        "out_data = out_data.drop(['WAP001', 'WAP002', 'WAP003', 'WAP004', 'WAP005', 'WAP006', 'WAP007', 'WAP008', 'WAP009', 'WAP010', 'WAP011', 'WAP012', 'WAP013', 'WAP014', 'WAP015', 'WAP016', 'WAP017', 'WAP018', 'WAP019', 'WAP020', 'WAP021', 'WAP022', 'WAP023', 'WAP024', 'WAP025', 'WAP026', 'WAP027', 'WAP028', 'WAP029', 'WAP030', 'WAP031', 'WAP032', 'WAP033', 'WAP034', 'WAP035', 'WAP036', 'WAP037', 'WAP038', 'WAP039', 'WAP040', 'WAP041', 'WAP042', 'WAP043', 'WAP044', 'WAP045', 'WAP046', 'WAP047', 'WAP048', 'WAP049', 'WAP050', 'WAP051', 'WAP052', 'WAP053', 'WAP054', 'WAP055', 'WAP056', 'WAP057', 'WAP058', 'WAP059', 'WAP060', 'WAP061', 'WAP062', 'WAP063', 'WAP064', 'WAP065', 'WAP066', 'WAP067', 'WAP068', 'WAP069', 'WAP070', 'WAP071', 'WAP072', 'WAP073', 'WAP074', 'WAP075', 'WAP076', 'WAP077', 'WAP078', 'WAP079', 'WAP080', 'WAP081', 'WAP082', 'WAP083', 'WAP084', 'WAP085', 'WAP086', 'WAP087', 'WAP088', 'WAP089', 'WAP090', 'WAP091', 'WAP092', 'WAP093', 'WAP094', 'WAP095', 'WAP096', 'WAP097', 'WAP098', 'WAP099', 'WAP100', 'WAP101', 'WAP102', 'WAP103', 'WAP104', 'WAP105', 'WAP106', 'WAP107', 'WAP108', 'WAP109', 'WAP110', 'WAP111', 'WAP112', 'WAP113', 'WAP114', 'WAP115', 'WAP116', 'WAP117', 'WAP118', 'WAP119', 'WAP120', 'WAP121', 'WAP122', 'WAP123', 'WAP124', 'WAP125', 'WAP126', 'WAP127', 'WAP128', 'WAP129', 'WAP130', 'WAP131', 'WAP132', 'WAP133', 'WAP134', 'WAP135', 'WAP136', 'WAP137', 'WAP138', 'WAP139', 'WAP140', 'WAP141', 'WAP142', 'WAP143', 'WAP144', 'WAP145', 'WAP146', 'WAP147', 'WAP148', 'WAP149', 'WAP150', 'WAP151', 'WAP152', 'WAP153', 'WAP154', 'WAP155', 'WAP156', 'WAP157', 'WAP158', 'WAP159', 'WAP160', 'WAP161', 'WAP162', 'WAP163', 'WAP164', 'WAP165', 'WAP166', 'WAP167', 'WAP168', 'WAP169', 'WAP170', 'WAP171', 'WAP172', 'WAP173', 'WAP174', 'WAP175', 'WAP176', 'WAP177', 'WAP178', 'WAP179', 'WAP180', 'WAP181', 'WAP182', 'WAP183', 'WAP184', 'WAP185', 'WAP186', 'WAP187', 'WAP188', 'WAP189', 'WAP190', 'WAP191', 'WAP192', 'WAP193', 'WAP194', 'WAP195', 'WAP196', 'WAP197', 'WAP198', 'WAP199', 'WAP200', 'WAP201', 'WAP202', 'WAP203', 'WAP204', 'WAP205', 'WAP206', 'WAP207', 'WAP208', 'WAP209', 'WAP210', 'WAP211', 'WAP212', 'WAP213', 'WAP214', 'WAP215', 'WAP216', 'WAP217', 'WAP218', 'WAP219', 'WAP220', 'WAP221', 'WAP222', 'WAP223', 'WAP224', 'WAP225', 'WAP226', 'WAP227', 'WAP228', 'WAP229', 'WAP230', 'WAP231', 'WAP232', 'WAP233', 'WAP234', 'WAP235', 'WAP236', 'WAP237', 'WAP238', 'WAP239', 'WAP240', 'WAP241', 'WAP242', 'WAP243', 'WAP244', 'WAP245', 'WAP246', 'WAP247', 'WAP248', 'WAP249', 'WAP250', 'WAP251', 'WAP252', 'WAP253', 'WAP254', 'WAP255', 'WAP256', 'WAP257', 'WAP258', 'WAP259', 'WAP260', 'WAP261', 'WAP262', 'WAP263', 'WAP264', 'WAP265', 'WAP266', 'WAP267', 'WAP268', 'WAP269', 'WAP270', 'WAP271', 'WAP272', 'WAP273', 'WAP274', 'WAP275', 'WAP276', 'WAP277', 'WAP278', 'WAP279', 'WAP280', 'WAP281', 'WAP282', 'WAP283', 'WAP284', 'WAP285', 'WAP286', 'WAP287', 'WAP288', 'WAP289', 'WAP290', 'WAP291', 'WAP292', 'WAP293', 'WAP294', 'WAP295', 'WAP296', 'WAP297', 'WAP298', 'WAP299', 'WAP300', 'WAP301', 'WAP302', 'WAP303', 'WAP304', 'WAP305', 'WAP306', 'WAP307', 'WAP308', 'WAP309', 'WAP310', 'WAP311', 'WAP312', 'WAP313', 'WAP314', 'WAP315', 'WAP316', 'WAP317', 'WAP318', 'WAP319', 'WAP320', 'WAP321', 'WAP322', 'WAP323', 'WAP324', 'WAP325', 'WAP326', 'WAP327', 'WAP328', 'WAP329', 'WAP330', 'WAP331', 'WAP332', 'WAP333', 'WAP334', 'WAP335', 'WAP336', 'WAP337', 'WAP338', 'WAP339', 'WAP340', 'WAP341', 'WAP342', 'WAP343', 'WAP344', 'WAP345', 'WAP346', 'WAP347', 'WAP348', 'WAP349', 'WAP350', 'WAP351', 'WAP352', 'WAP353', 'WAP354', 'WAP355', 'WAP356', 'WAP357', 'WAP358', 'WAP359', 'WAP360', 'WAP361', 'WAP362', 'WAP363', 'WAP364', 'WAP365', 'WAP366', 'WAP367', 'WAP368', 'WAP369', 'WAP370', 'WAP371', 'WAP372', 'WAP373', 'WAP374', 'WAP375', 'WAP376', 'WAP377', 'WAP378', 'WAP379', 'WAP380', 'WAP381', 'WAP382', 'WAP383', 'WAP384', 'WAP385', 'WAP386', 'WAP387', 'WAP388', 'WAP389', 'WAP390', 'WAP391', 'WAP392', 'WAP393', 'WAP394', 'WAP395', 'WAP396', 'WAP397', 'WAP398', 'WAP399', 'WAP400', 'WAP401', 'WAP402', 'WAP403', 'WAP404', 'WAP405', 'WAP406', 'WAP407', 'WAP408', 'WAP409', 'WAP410', 'WAP411', 'WAP412', 'WAP413', 'WAP414', 'WAP415', 'WAP416', 'WAP417', 'WAP418', 'WAP419', 'WAP420', 'WAP421', 'WAP422', 'WAP423', 'WAP424', 'WAP425', 'WAP426', 'WAP427', 'WAP428', 'WAP429', 'WAP430', 'WAP431', 'WAP432', 'WAP433', 'WAP434', 'WAP435', 'WAP436', 'WAP437', 'WAP438', 'WAP439', 'WAP440', 'WAP441', 'WAP442', 'WAP443', 'WAP444', 'WAP445', 'WAP446', 'WAP447', 'WAP448', 'WAP449', 'WAP450', 'WAP451', 'WAP452', 'WAP453', 'WAP454', 'WAP455', 'WAP456', 'WAP457', 'WAP458', 'WAP459', 'WAP460', 'WAP461', 'WAP462', 'WAP463', 'WAP464', 'WAP465', 'WAP466', 'WAP467', 'WAP468', 'WAP469', 'WAP470', 'WAP471', 'WAP472', 'WAP473', 'WAP474', 'WAP475', 'WAP476', 'WAP477', 'WAP478', 'WAP479', 'WAP480', 'WAP481', 'WAP482', 'WAP483', 'WAP484', 'WAP485', 'WAP486', 'WAP487', 'WAP488', 'WAP489', 'WAP490', 'WAP491', 'WAP492', 'WAP493', 'WAP494', 'WAP495', 'WAP496', 'WAP497', 'WAP498', 'WAP499', 'WAP500', 'WAP501', 'WAP502', 'WAP503', 'WAP504', 'WAP505', 'WAP506', 'WAP507', 'WAP508', 'WAP509', 'WAP510', 'WAP511', 'WAP512', 'WAP513', 'WAP514', 'WAP515', 'WAP516', 'WAP517', 'WAP518', 'WAP519', 'WAP520','SPACEID'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm21uz4Udajs"
      },
      "source": [
        "print(list(out_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGd9ss29DHeS"
      },
      "source": [
        "in_data = train_data.copy()\n",
        "in_data = in_data.drop(['LONGITUDE', 'LATITUDE', 'FLOOR', 'BUILDINGID', 'SPACEID', 'RELATIVEPOSITION'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHs81lxBE7pP"
      },
      "source": [
        "print(list(in_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpan4iJQDtf7"
      },
      "source": [
        "in_data.shape, out_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eut5RNvWhJq6"
      },
      "source": [
        "# Feature Importance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1rOrTm1dyIe"
      },
      "source": [
        "# Feature Importances\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "feature_names = ['WAP001', 'WAP002', 'WAP003', 'WAP004', 'WAP005', 'WAP006', 'WAP007', 'WAP008', 'WAP009', 'WAP010', 'WAP011', 'WAP012', 'WAP013', 'WAP014', 'WAP015', 'WAP016', 'WAP017', 'WAP018', 'WAP019', 'WAP020', 'WAP021', 'WAP022', 'WAP023', 'WAP024', 'WAP025', 'WAP026', 'WAP027', 'WAP028', 'WAP029', 'WAP030', 'WAP031', 'WAP032', 'WAP033', 'WAP034', 'WAP035', 'WAP036', 'WAP037', 'WAP038', 'WAP039', 'WAP040', 'WAP041', 'WAP042', 'WAP043', 'WAP044', 'WAP045', 'WAP046', 'WAP047', 'WAP048', 'WAP049', 'WAP050', 'WAP051', 'WAP052', 'WAP053', 'WAP054', 'WAP055', 'WAP056', 'WAP057', 'WAP058', 'WAP059', 'WAP060', 'WAP061', 'WAP062', 'WAP063', 'WAP064', 'WAP065', 'WAP066', 'WAP067', 'WAP068', 'WAP069', 'WAP070', 'WAP071', 'WAP072', 'WAP073', 'WAP074', 'WAP075', 'WAP076', 'WAP077', 'WAP078', 'WAP079', 'WAP080', 'WAP081', 'WAP082', 'WAP083', 'WAP084', 'WAP085', 'WAP086', 'WAP087', 'WAP088', 'WAP089', 'WAP090', 'WAP091', 'WAP092', 'WAP093', 'WAP094', 'WAP095', 'WAP096', 'WAP097', 'WAP098', 'WAP099', 'WAP100', 'WAP101', 'WAP102', 'WAP103', 'WAP104', 'WAP105', 'WAP106', 'WAP107', 'WAP108', 'WAP109', 'WAP110', 'WAP111', 'WAP112', 'WAP113', 'WAP114', 'WAP115', 'WAP116', 'WAP117', 'WAP118', 'WAP119', 'WAP120', 'WAP121', 'WAP122', 'WAP123', 'WAP124', 'WAP125', 'WAP126', 'WAP127', 'WAP128', 'WAP129', 'WAP130', 'WAP131', 'WAP132', 'WAP133', 'WAP134', 'WAP135', 'WAP136', 'WAP137', 'WAP138', 'WAP139', 'WAP140', 'WAP141', 'WAP142', 'WAP143', 'WAP144', 'WAP145', 'WAP146', 'WAP147', 'WAP148', 'WAP149', 'WAP150', 'WAP151', 'WAP152', 'WAP153', 'WAP154', 'WAP155', 'WAP156', 'WAP157', 'WAP158', 'WAP159', 'WAP160', 'WAP161', 'WAP162', 'WAP163', 'WAP164', 'WAP165', 'WAP166', 'WAP167', 'WAP168', 'WAP169', 'WAP170', 'WAP171', 'WAP172', 'WAP173', 'WAP174', 'WAP175', 'WAP176', 'WAP177', 'WAP178', 'WAP179', 'WAP180', 'WAP181', 'WAP182', 'WAP183', 'WAP184', 'WAP185', 'WAP186', 'WAP187', 'WAP188', 'WAP189', 'WAP190', 'WAP191', 'WAP192', 'WAP193', 'WAP194', 'WAP195', 'WAP196', 'WAP197', 'WAP198', 'WAP199', 'WAP200', 'WAP201', 'WAP202', 'WAP203', 'WAP204', 'WAP205', 'WAP206', 'WAP207', 'WAP208', 'WAP209', 'WAP210', 'WAP211', 'WAP212', 'WAP213', 'WAP214', 'WAP215', 'WAP216', 'WAP217', 'WAP218', 'WAP219', 'WAP220', 'WAP221', 'WAP222', 'WAP223', 'WAP224', 'WAP225', 'WAP226', 'WAP227', 'WAP228', 'WAP229', 'WAP230', 'WAP231', 'WAP232', 'WAP233', 'WAP234', 'WAP235', 'WAP236', 'WAP237', 'WAP238', 'WAP239', 'WAP240', 'WAP241', 'WAP242', 'WAP243', 'WAP244', 'WAP245', 'WAP246', 'WAP247', 'WAP248', 'WAP249', 'WAP250', 'WAP251', 'WAP252', 'WAP253', 'WAP254', 'WAP255', 'WAP256', 'WAP257', 'WAP258', 'WAP259', 'WAP260', 'WAP261', 'WAP262', 'WAP263', 'WAP264', 'WAP265', 'WAP266', 'WAP267', 'WAP268', 'WAP269', 'WAP270', 'WAP271', 'WAP272', 'WAP273', 'WAP274', 'WAP275', 'WAP276', 'WAP277', 'WAP278', 'WAP279', 'WAP280', 'WAP281', 'WAP282', 'WAP283', 'WAP284', 'WAP285', 'WAP286', 'WAP287', 'WAP288', 'WAP289', 'WAP290', 'WAP291', 'WAP292', 'WAP293', 'WAP294', 'WAP295', 'WAP296', 'WAP297', 'WAP298', 'WAP299', 'WAP300', 'WAP301', 'WAP302', 'WAP303', 'WAP304', 'WAP305', 'WAP306', 'WAP307', 'WAP308', 'WAP309', 'WAP310', 'WAP311', 'WAP312', 'WAP313', 'WAP314', 'WAP315', 'WAP316', 'WAP317', 'WAP318', 'WAP319', 'WAP320', 'WAP321', 'WAP322', 'WAP323', 'WAP324', 'WAP325', 'WAP326', 'WAP327', 'WAP328', 'WAP329', 'WAP330', 'WAP331', 'WAP332', 'WAP333', 'WAP334', 'WAP335', 'WAP336', 'WAP337', 'WAP338', 'WAP339', 'WAP340', 'WAP341', 'WAP342', 'WAP343', 'WAP344', 'WAP345', 'WAP346', 'WAP347', 'WAP348', 'WAP349', 'WAP350', 'WAP351', 'WAP352', 'WAP353', 'WAP354', 'WAP355', 'WAP356', 'WAP357', 'WAP358', 'WAP359', 'WAP360', 'WAP361', 'WAP362', 'WAP363', 'WAP364', 'WAP365', 'WAP366', 'WAP367', 'WAP368', 'WAP369', 'WAP370', 'WAP371', 'WAP372', 'WAP373', 'WAP374', 'WAP375', 'WAP376', 'WAP377', 'WAP378', 'WAP379', 'WAP380', 'WAP381', 'WAP382', 'WAP383', 'WAP384', 'WAP385', 'WAP386', 'WAP387', 'WAP388', 'WAP389', 'WAP390', 'WAP391', 'WAP392', 'WAP393', 'WAP394', 'WAP395', 'WAP396', 'WAP397', 'WAP398', 'WAP399', 'WAP400', 'WAP401', 'WAP402', 'WAP403', 'WAP404', 'WAP405', 'WAP406', 'WAP407', 'WAP408', 'WAP409', 'WAP410', 'WAP411', 'WAP412', 'WAP413', 'WAP414', 'WAP415', 'WAP416', 'WAP417', 'WAP418', 'WAP419', 'WAP420', 'WAP421', 'WAP422', 'WAP423', 'WAP424', 'WAP425', 'WAP426', 'WAP427', 'WAP428', 'WAP429', 'WAP430', 'WAP431', 'WAP432', 'WAP433', 'WAP434', 'WAP435', 'WAP436', 'WAP437', 'WAP438', 'WAP439', 'WAP440', 'WAP441', 'WAP442', 'WAP443', 'WAP444', 'WAP445', 'WAP446', 'WAP447', 'WAP448', 'WAP449', 'WAP450', 'WAP451', 'WAP452', 'WAP453', 'WAP454', 'WAP455', 'WAP456', 'WAP457', 'WAP458', 'WAP459', 'WAP460', 'WAP461', 'WAP462', 'WAP463', 'WAP464', 'WAP465', 'WAP466', 'WAP467', 'WAP468', 'WAP469', 'WAP470', 'WAP471', 'WAP472', 'WAP473', 'WAP474', 'WAP475', 'WAP476', 'WAP477', 'WAP478', 'WAP479', 'WAP480', 'WAP481', 'WAP482', 'WAP483', 'WAP484', 'WAP485', 'WAP486', 'WAP487', 'WAP488', 'WAP489', 'WAP490', 'WAP491', 'WAP492', 'WAP493', 'WAP494', 'WAP495', 'WAP496', 'WAP497', 'WAP498', 'WAP499', 'WAP500', 'WAP501', 'WAP502', 'WAP503', 'WAP504', 'WAP505', 'WAP506', 'WAP507', 'WAP508', 'WAP509', 'WAP510', 'WAP511', 'WAP512', 'WAP513', 'WAP514', 'WAP515', 'WAP516', 'WAP517', 'WAP518', 'WAP519', 'WAP520', 'LONGITUDE', 'LATITUDE', 'FLOOR', 'BUILDINGID', 'SPACEID', 'RELATIVEPOSITION', 'USERID', 'PHONEID', 'TIMESTAMP']\n",
        "\n",
        "# Building  a classification task using 3 informative features\n",
        "Xe, ye = make_classification(n_samples=19937,\n",
        "                           n_features=529,\n",
        "                           n_informative=5,\n",
        "                           n_redundant=0,\n",
        "                           n_repeated=0,\n",
        "                           n_classes=2,\n",
        "                           random_state=0,\n",
        "                           shuffle=False)\n",
        "\n",
        "# Building a forest and compute the feature importances\n",
        "forest = ExtraTreesClassifier(n_estimators=250,\n",
        "                              random_state=0)\n",
        "plt.rcParams[\"figure.figsize\"] = (28,250)\n",
        "forest.fit(Xe, ye)\n",
        "importances = forest.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
        "             axis=0)\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "# Plotting the feature importances of the forest\n",
        "plt.figure()\n",
        "plt.title(\"Feature importances\")\n",
        "plt.barh(range(Xe.shape[1]), importances[indices],\n",
        "       color=\"r\", xerr=std[indices], align=\"center\")\n",
        "plt.yticks(range(Xe.shape[1]), [feature_names[i] for i in indices])\n",
        "plt.ylim([-1, Xe.shape[1]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwsbyQfxknqU"
      },
      "source": [
        "Checking for Missing Values in all columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh6edwRKkmgB"
      },
      "source": [
        "train_data.isnull().sum().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoNk_JqUmVx8"
      },
      "source": [
        "# Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aneWyFJAzj9"
      },
      "source": [
        "Output arrays for all predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db18Wje0Ftys"
      },
      "source": [
        "# Output arrays for all predictions\n",
        "\n",
        "# for Lat & Long\n",
        "\n",
        "y_lat = out_data.copy()\n",
        "y_lat = y_lat.drop(['FLOOR', 'BUILDINGID', 'RELATIVEPOSITION'], axis = 1)\n",
        "y_lat = lil_matrix(y_lat).toarray()\n",
        "\n",
        "#for Relative position \n",
        "\n",
        "y_rp = out_data.copy()\n",
        "y_rp = y_rp.drop(['LONGITUDE', 'LATITUDE', 'FLOOR', 'BUILDINGID',], axis = 1)\n",
        "y_rp = lil_matrix(y_rp).toarray()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMrlsC01FMr1"
      },
      "source": [
        "# Normalizing the in_data \n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(in_data)    \n",
        "X = scaler.transform(in_data)\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apbcvNrQHlDc"
      },
      "source": [
        "y_lat.shape,y_rp.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uffrquw8YXlw"
      },
      "source": [
        "# Predicting Latitude and Longitude (LAT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zulr5HwNHu0R"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(in_data, y_lat, test_size=0.25, random_state=45) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwVLp75mI3L-"
      },
      "source": [
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6KsbiRlYcwW"
      },
      "source": [
        " Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx4bkj5mNINa"
      },
      "source": [
        "Using Raw data for prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfVGyO3kIAJM"
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# Define the model\n",
        "model_3 = Sequential()\n",
        "model_3.add(Dense(300, input_dim=520, activation='relu'))\n",
        "model_3.add(BatchNormalization())\n",
        "model_3.add(Dropout(0.2))\n",
        "model_3.add(Dense(300, activation='relu'))\n",
        "model_3.add(BatchNormalization())\n",
        "model_3.add(Dropout(0.2))\n",
        "model_3.add(Dense(300, activation='relu'))\n",
        "model_3.add(BatchNormalization())\n",
        "model_3.add(Dense(2, activation='linear'))\n",
        "model_3.compile(loss='mean_absolute_error', optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model_3.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNnvwxNRJElf"
      },
      "source": [
        "monitor2 = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto')\n",
        "hist_latlong=model_3.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=1024,\n",
        "    validation_split = 0.4,\n",
        "    epochs=200,\n",
        "    shuffle=True,\n",
        "    callbacks=[monitor2],\n",
        "    verbose=2,\n",
        "    validation_data=(x_test,y_test)\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeKYQByqJUaX"
      },
      "source": [
        "pred = model_3.predict(x_test)\n",
        "\n",
        "pred = np.argmax(pred,axis=1) # raw probabilities to choose class (highest probability)\n",
        "\n",
        "\n",
        "y_true= np.argmax(y_test,axis=1) \n",
        "sac1 = metrics.accuracy_score(y_true, pred)\n",
        "print(\"Accuracy score: {}\".format(sac1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzmn8yZ8JsMY"
      },
      "source": [
        "predictions3 = (model_3.predict(x_test))\n",
        "\n",
        "# accuracy\n",
        "print(\"RMSE of predicting LONGTITUDE = \", mean_absolute_error(y_test[:,0],predictions3[:,0]))\n",
        "print(\"RMSE of predicting LATITUDE = \", mean_absolute_error(y_test[:,1],predictions3[:,1]))\n",
        "\n",
        "\n",
        "print(\"--- Run time: %s mins ---\" % np.round(((time.time() - start_time)/60),2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSuXDaiKKlv8"
      },
      "source": [
        "print(hist_latlong.history.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IueTXFT5Xlum"
      },
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (6, 3)\n",
        "plt.plot(hist_latlong.history['accuracy'])\n",
        "plt.plot(hist_latlong.history['val_accuracy'])\n",
        "plt.title('Model Accuracy for predicting Latitude and Longitude')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3VG_H0KM0Tt"
      },
      "source": [
        "*Normalized Model*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMj_j5urVCb2"
      },
      "source": [
        "# Normalized\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y_lat, test_size=0.3, random_state=45) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3vEHZKFMzeT"
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# Define the model\n",
        "model_3 = Sequential()\n",
        "model_3.add(Dense(300, input_dim=520, activation='relu'))\n",
        "model_3.add(BatchNormalization())\n",
        "model_3.add(Dropout(0.2))\n",
        "model_3.add(Dense(150, activation='relu'))\n",
        "model_3.add(BatchNormalization())\n",
        "model_3.add(Dropout(0.2))\n",
        "model_3.add(Dense(150, activation='relu'))\n",
        "model_3.add(BatchNormalization())\n",
        "model_3.add(Dense(2, activation='linear'))\n",
        "model_3.compile(loss='mean_absolute_error', optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model_3.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEbEaUjx2OC4"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model_3, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW0vsI2kMzeZ"
      },
      "source": [
        "monitor2 = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto')\n",
        "hist_latlong=model_3.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=1024,\n",
        "    validation_split = 0.4,\n",
        "    epochs=200,\n",
        "    shuffle=True,\n",
        "    callbacks=[monitor2],\n",
        "    verbose=2,\n",
        "    validation_data=(x_test,y_test)\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "753Ak0FBMzea"
      },
      "source": [
        "pred = model_3.predict(x_test)\n",
        "\n",
        "pred = np.argmax(pred,axis=1) # raw probabilities to choose class (highest probability)\n",
        "\n",
        "\n",
        "y_true= np.argmax(y_test,axis=1) \n",
        "sac1 = metrics.accuracy_score(y_true, pred)\n",
        "print(\"Accuracy score: {}\".format(sac1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUgMq4FwMzec"
      },
      "source": [
        "predictions3 = (model_3.predict(x_test))\n",
        "\n",
        "# accuracy\n",
        "print(\"RMSE of predicting LONGTITUDE = \", mean_absolute_error(y_test[:,0],predictions3[:,0]))\n",
        "print(\"RMSE of predicting LATITUDE = \", mean_absolute_error(y_test[:,1],predictions3[:,1]))\n",
        "\n",
        "\n",
        "print(\"--- Run time: %s mins ---\" % np.round(((time.time() - start_time)/60),2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHa2vpH6Mzee"
      },
      "source": [
        "print(hist_latlong.history.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI2w821sMzef"
      },
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (6, 3)\n",
        "plt.plot(hist_latlong.history['accuracy'])\n",
        "plt.plot(hist_latlong.history['val_accuracy'])\n",
        "plt.title('Model Accuracy for predicting Latitude and Longitude(Normalised Model)')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QShvjmuZY0Q6"
      },
      "source": [
        "# Predicting Relative Position (RP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CW7Zs-3NOIF"
      },
      "source": [
        "x_train1, x_test1, y_train1, y_test1 = train_test_split(X, y_rp, test_size=0.3, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoBis7kU4YE2"
      },
      "source": [
        "### Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hF8KMki4fld"
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# Define the model\n",
        "model_rp = Sequential()\n",
        "model_rp.add(Dense(300, input_dim=520, activation='relu'))\n",
        "model_rp.add(BatchNormalization())\n",
        "model_rp.add(Dropout(0.3))\n",
        "model_rp.add(Dense(150, activation='relu'))\n",
        "model_rp.add(BatchNormalization())\n",
        "model_rp.add(Dropout(0.3))\n",
        "model_rp.add(Dense(300, activation='relu'))\n",
        "model_rp.add(BatchNormalization())\n",
        "model_rp.add(Dense(2, activation='linear'))\n",
        "model_rp.compile(loss='mean_absolute_error', optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model_rp.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UePouN94fao"
      },
      "source": [
        "monitor2 = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=7, verbose=1, mode='auto')\n",
        "hist_rp=model_rp.fit(\n",
        "    x_train1,\n",
        "    y_train1,\n",
        "    batch_size=700,\n",
        "    validation_split = 0.2,\n",
        "    epochs=150,\n",
        "    shuffle=True,\n",
        "    callbacks=[monitor2],\n",
        "    verbose=2,\n",
        "    validation_data=(x_test1,y_test1)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRDXFyoj4fSq"
      },
      "source": [
        "\n",
        "pred = model_rp.predict(x_test1)\n",
        "\n",
        "pred = np.argmax(pred,axis=1) # raw probabilities to choose class (highest probability)\n",
        "\n",
        "y_true= np.argmax(y_test1,axis=1) \n",
        "sac1 = metrics.accuracy_score(y_true, pred)\n",
        "print(\"Accuracy score: {}\".format(sac1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKDvluXQJCK0"
      },
      "source": [
        "\n",
        "**other addtional models that may complement this basic location and localization model are**\n",
        "\n",
        "*   a model to predict next location and calculate the distance beetween two adjacent location\n",
        "*   home and work location prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcRUgiUWGzWZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Foei37sPGz0W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1uSVJX2G31j"
      },
      "source": [
        "SAVING MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNYJ7XlPHV5G",
        "outputId": "fddac2a6-83aa-495c-fe92-08939be0ce59"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFxE_jAcGY5e"
      },
      "source": [
        "# # loading model\n",
        "# from tensorflow.keras.models import load_model\n",
        "# model = load_model('/content/drive/MyDrive/Colab Notebooks/kbsAssignmentTwo/vgg16Model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}