{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [[ ' this ' , ' is ' , ' the ' , ' first ' , ' sentence ' , ' for ' , ' word2vec ' ],\n",
    "[ ' this ' , ' is ' , ' the ' , ' second ' , ' sentence ' ], [ ' yet ' , ' another ' , ' sentence ' ],\n",
    "[ ' one ' , ' more ' , ' sentence ' ], [ ' and ' , ' the ' , ' final ' , ' sentence ' ]]\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "print(model)\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "print(model[ ' sentence ' ])\n",
    "model.save( ' model.bin ' )\n",
    "new_model = Word2Vec.load( ' model.bin ' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = ' glove.6B.100d.txt '\n",
    "word2vec_output_file = ' glove.6B.100d.txt.word2vec '\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can load it and perform the same (king - man) + woman = ?\n",
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model\n",
    "filename = ' glove.6B.100d.txt.word2vec '\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=[ ' woman ' , ' king ' ], negative=[ ' man ' ], topn=1)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each text document is classified as positive 1 or negative 0.\n",
    "docs = [ ' Well done! ' , ' Good work ' , ' Great effort ' , ' nice work ' , ' Excellent! ' , ' Weak ' ,\n",
    "' Poor effort! ' , ' not good ' , ' poor work ' , ' Could have done better. ' ]\n",
    "# define class labels\n",
    "labels = [1,1,1,1,1,0,0,0,0,0]\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)\n",
    "The sequences have different lengths and Keras prefers inputs to be vectorized and all inputs to have the same length. We will pad all input sequences to have the length of 4\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding= ' post ' )\n",
    "print(padded_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We flatten this to a one 32-element vector to pass on to the Dense output layer.\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation= ' sigmoid ' ))\n",
    "model.compile(optimizer= ' adam ' , loss= ' binary_crossentropy ' , metrics=[ ' acc ' ])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of Learning an Embedding\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print( ' Accuracy: %f ' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of Using Pre-Trained GloVe Embedding\n",
    "The Keras Embedding layer can also use a word embedding learned elsewhere\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding= ' post ' )\n",
    "print(padded_docs)\n",
    "#Next, we need to load the entire GloVe word embedding file into memory as a dictionary of word to embedding array.\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open( ' glove.6B.100d.txt ' )\n",
    "for line in f:\n",
    "values = line.split()\n",
    "word = values[0]\n",
    "coefs = asarray(values[1:], dtype= ' float32 ' )\n",
    "embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print( ' Loaded %s word vectors. ' % len(embeddings_index))\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "embedding_vector = embeddings_index.get(word)\n",
    "if embedding_vector is not None:\n",
    "embedding_matrix[i] = embedding_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# complete worked example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation= ' sigmoid ' ))\n",
    "model.compile(optimizer= ' adam ' , loss= ' binary_crossentropy ' , metrics=[ ' acc ' ])\n",
    "model.summary()\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print( ' Accuracy: %f ' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['warm' 'hot' 'cold' 'luke warm' 'hot' 'cold' 'warm']\n",
      "[3 1 0 2 1 0 3]\n",
      "cold\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# define example\n",
    "data = ['warm','hot','cold','luke warm','hot','cold','warm']\n",
    "values = array(data)\n",
    "print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "inverted = label_encoder.inverse_transform(integer_encoded)[2]\n",
    "print(inverted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
