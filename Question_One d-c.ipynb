{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Question_One d-c.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eJ5wiKn35k7F","outputId":"206d201f-2bc4-4c73-a75a-205fe2bb915c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"Mvk0jasvQfbp"},"source":["\n","\n","r178459r\n","mujati kundai\n","\n"]},{"cell_type":"code","metadata":{"id":"Bosl7LWYQfEh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NyqO8yKn5z4u"},"source":["a) "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7O0UJxs-6znM","outputId":"8d14c5ca-d995-474b-e0ff-573457311be5"},"source":["pip install sklearn tqdm numpy tensorflow"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.6.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.39.0)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n","Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (5.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n","Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n","Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n","Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.5)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.34.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.6.4)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.5.0)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n"]}]},{"cell_type":"code","metadata":{"id":"7ROn23JZ7H2X"},"source":["import time\n","import pickle\n","import tensorflow as tf\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","    # only use GPU memory that we need, not allocate all the GPU memory\n","    tf.config.experimental.set_memory_growth(gpus[0], enable=True)\n","\n","import tqdm\n","import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.metrics import Recall, Precision"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AKOPvqU-7P70"},"source":["SEQUENCE_LENGTH = 100 # the length of all sequences (number of words per sample)\n","EMBEDDING_SIZE = 100  # Using 100-Dimensional GloVe embedding vectors\n","TEST_SIZE = 0.25 # ratio of testing set\n","\n","BATCH_SIZE = 64\n","EPOCHS = 10 # number of epochs\n","\n","label2int = {\"ham\": 0, \"spam\": 1}\n","int2label = {0: \"ham\", 1: \"spam\"}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fmWBvBEG7af3"},"source":["Loading dataset"]},{"cell_type":"code","metadata":{"id":"ZU5kHsyJ7UXk"},"source":["\n","def load_data():\n","    \n"," \n","\n","with open(\"content/drive/My Drive/Colab Notebooks/spam_or_not_spam.csv\"):\n","        for line in f:\n","            split = line.split()\n","            labels.append(split[0].strip())\n","            texts.append(' '.join(split[1:]).strip())\n","return texts, labels\n","\n","text = file.read()\n","file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5sZerK0FCSVW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0XHqfrMDCT6t"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dVc6MbpW75aL"},"source":["Calling function"]},{"cell_type":"code","metadata":{"id":"9s63mbBB78D4"},"source":["# load the data\n","X, y = load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VFBWjJMyCVUB"},"source":["\n","# vectorizing text, turning each text into sequence of integers\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X)\n","\n","\n","# lets dump it to a file, so we can use it in testing\n","pickle.dump(tokenizer, open(\"results/tokenizer.pickle\", \"wb\"))\n","# convert to sequence of integers\n","X = tokenizer.texts_to_sequences(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w3MbvVdsKAvt"},"source":["preparing the dataset"]},{"cell_type":"code","metadata":{"id":"0EYtZW69KDRG"},"source":["# Text tokenization\n","# vectorizing text, turning each text into sequence of integers\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X)\n","# lets dump it to a file, so we can use it in testing\n","pickle.dump(tokenizer, open(\"results/tokenizer.pickle\", \"wb\"))\n","# convert to sequence of integers\n","X = tokenizer.texts_to_sequences(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L3ebEVfrKRO1"},"source":["y = [ label2int[label] for label in y ]\n","y = to_categorical(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ka8curuIMkDR"},"source":["a)Splitting into test  and train sets"]},{"cell_type":"code","metadata":{"id":"m4Ov-3MRKZZD"},"source":["# split and shuffle\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=7)\n","# print our data shapes\n","print(\"X_train.shape:\", X_train.shape)\n","print(\"X_test.shape:\", X_test.shape)\n","print(\"y_train.shape:\", y_train.shape)\n","print(\"y_test.shape:\", y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QK-o9N3WKvlq"},"source":["with open(f\"data/glove.6B.{dim}d.txt\", encoding='utf8') as f:\n","        for line in tqdm.tqdm(f, \"Reading GloVe\"):\n","            values = line.split()\n","            word = values[0]\n","            vectors = np.asarray(values[1:], dtype='float32')\n","            embedding_index[word] = vectors\n","\n","    word_index = tokenizer.word_index\n","    embedding_matrix = np.zeros((len(word_index)+1, dim))\n","    for word, i in word_index.items():\n","        embedding_vector = embedding_index.get(word)\n","        if embedding_vector is not None:\n","            # words not found will be 0s\n","            embedding_matrix[i] = embedding_vector\n","            \n","    return embedding_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lcc35VlPMyz3"},"source":["d) Building an LSTM Model based on word embedding"]},{"cell_type":"code","metadata":{"id":"BOvpj2fdLHJE"},"source":["def get_model(tokenizer, lstm_units):\n","    \n","    # get the GloVe embedding vectors\n","    embedding_matrix = get_embedding_vectors(tokenizer)\n","    model = Sequential()\n","    model.add(Embedding(len(tokenizer.word_index)+1,\n","              EMBEDDING_SIZE,\n","              weights=[embedding_matrix],\n","              trainable=False,\n","              input_length=SEQUENCE_LENGTH))\n","\n","    model.add(LSTM(lstm_units, recurrent_dropout=0.2))\n","    model.add(Dropout(0.3))\n","    model.add(Dense(2, activation=\"softmax\"))\n","    # compile as rmsprop optimizer\n","    # aswell as with recall metric\n","    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n","                  metrics=[\"accuracy\", keras_metrics.precision(), keras_metrics.recall()])\n","    model.summary()\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q6cNa3YzMxIZ"},"source":["constructs the model with 128 LSTM units"]},{"cell_type":"code","metadata":{"id":"VmPUYJ_vLOWm"},"source":["# constructs the model with 128 LSTM units\n","model = get_model(tokenizer=tokenizer, lstm_units=128)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dYWqYWkDNQzm"},"source":["e) Training the Model"]},{"cell_type":"code","metadata":{"id":"P67i7HHuLR7H"},"source":["# initialize our ModelCheckpoint and TensorBoard callbacks\n","# model checkpoint for saving best weights\n","model_checkpoint = ModelCheckpoint(\"results/spam_classifier_{val_loss:.2f}.h5\", save_best_only=True,\n","                                    verbose=1)\n","# for better visualization\n","tensorboard = TensorBoard(f\"logs/spam_classifier_{time.time()}\")\n","# train the model\n","model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","          batch_size=BATCH_SIZE, epochs=EPOCHS,\n","          callbacks=[tensorboard, model_checkpoint],\n","          verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3HZGa18dNK_z"},"source":["f) Evaluation Of Model"]},{"cell_type":"code","metadata":{"id":"MfMm5y7hLaQK"},"source":["# get the loss and metrics\n","result = model.evaluate(X_test, y_test)\n","# extract those\n","loss = result[0]\n","accuracy = result[1]\n","precision = result[2]\n","recall = result[3]\n","\n","print(f\"[+] Accuracy: {accuracy*100:.2f}%\")\n","print(f\"[+] Precision:   {precision*100:.2f}%\")\n","print(f\"[+] Recall:   {recall*100:.2f}%\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_nGi1eBELgdE"},"source":["def get_predictions(text):\n","    sequence = tokenizer.texts_to_sequences([text])\n","    # pad the sequence\n","    sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)\n","    # get the prediction\n","    prediction = model.predict(sequence)[0]\n","    # one-hot encoded vector, revert using np.argmax\n","    return int2label[np.argmax(prediction)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IH9FkV14LhfS"},"source":["text = \"You won a prize of 1,000$, click here to claim!\"\n","get_predictions(text)"],"execution_count":null,"outputs":[]}]}